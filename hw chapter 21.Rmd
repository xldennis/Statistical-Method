---
title: "homework chapter 20 - 22"
author: "xiang liu"
date: "April 8th, 2015"
output:
  html_document:
    fig_height: 4
    fig_width: 8
    toc: yes
  pdf_document:
    fig_height: 4.3
    fig_width: 8
    keep_tex: yes
    toc: yes
geometry: margin=1in
setspace: onehalfspacing
fontsize: 12pt
---


**Based on KNNL problem 20.2-20.4 on page 889**. Coin-operated terminals. A university computer service conducted an experiment in which one coin-operated computer terminal was placed at each of four different locations on the campus last semester during the midterm week and again during the final week of classes. The data that follow show the number of hours each terminal was NOT in use during the week at the four locations (factor A) and for the two different weeks (factor B).

\hfill

<!---
$\begin{tabular}{c c c} 
& \multicolumn{2} {c} {Factor B}\\ [0.5ex] 
\cline{2-3}
Factor A & j = 1 & j = 2\\
(location)&Midterm&Final\\
\hline 
i = 1 & 16.5 & 21.4\\
i = 2 & 11.8 & 17.3\\
i = 3 & 12.3 & 16.9\\
i = 4 & 16.6 & 21.0\\ 
\end{tabular}$
-->

<a href="http://tinypic.com?ref=15g88e8" target="_blank"><img src="http://i61.tinypic.com/15g88e8.png" border="0" alt="Image and video hosting by TinyPic"></a>

\hfill

Assume that no-interaction ANOVA model (20.1) is appropriate.

\hfill

**(a)**  Plot the dat in the format of Figure 20.1. Does it appear that interaction effects are present? Does it appear that factor A and factor B main effects are present?

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
library(pander)
library(doBy)
panderOptions('round', 2)
panderOptions('keep.trailing.zeros', F)

###SETTING OVER
data.21=read.table("CH20PR02.txt",col.names=c('Y','A','B'))
names(data.21)=c("time","location","week")
data.21$location = factor(data.21$location)
data.21$week = factor(data.21$week)

###PLOT THE CELL MEANS FOR TWO FACTORS  
# library(lattice)
# stripplot(time~location, groups=week, auto.key=list(space='right', text = c("week:1", "week:2"), points=F,lines=T), points = T, jitter.data=T, type=c("p","a"), data=data.21, xlab="Location", ylab="Number of hours not in use")
interaction.plot(data.21$week,data.21$location,data.21$time,trace.label =
                   'Location', xlab = 'Week', ylab = 'Average Number 
                 of Hours not in Use')
```

* No clear interaction effect presented since the four line seem to be parallel to each other. Changing location wouldn’t affect the relationship between week and average time of no in use. 
* It does appear to have main effects for factor A and B respectively. Lower average number of hours the terminal isn’t in use is in week 1 (vs week 2) and location 2 & 3 (vs location 1 & 4)

\hfill

**(b)**  Set up the ANOVA table.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
model = lm(time ~ location + week, data.21)
panderOptions('round', 4)
pander(anova(model))
```

\hfill

**(c)**   Obtain the fitted value for cell i = 3 and j = 2.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
new <- data.frame(location = factor(3), week = factor(2))
predict = predict(model,newdata=new)
# sprintf("The fitted value is %.3f", predict)
```

* $\hat \mu_{32} = \bar Y_{3.} + \bar Y_{.2} - \bar Y_{..}$ = 17.025

\hfill

**(d)**   Construct a 95% confidence interval for $\mu_{32}$.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
new <- data.frame(location = factor(3), week = factor(2))
predict = predict(model,newdata=new,interval = 'confidence', se.fit = TRUE)
critical.t = qt(0.975,df = 3)
value = predict$fit[1]
se = predict$se.fit
interval = predict$fit[2:3]
panderOptions('round', 2)
pander(cbind('Estimated Value' = value, 'Standard Error' = se, 
             't' = critical.t, '2.5%' = interval[1], '97.5%'=interval[2]))
```

\hfill

**(d)**   Carry out tests for location and week effects; use a level of significance $\alpha$ = 0.05. State your conclusions.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
panderOptions('round', 4)
pander(anova(model))
```

* Model: $y_{ij} = \mu + \alpha_i + \beta_j + \epsilon_{ij}$ where $\Sigma^4_{i = 1} \alpha_i = 0$ and $\Sigma^2_{j = 1} \beta_j = 0$ and $\epsilon_{ij} \sim  iid(0, \sigma^2)$

* Main effect for location
    + H0: all $\alpha$'s equal 0 vs Ha: at least one $\alpha_i \neq 0$  
    + Test Statistics: as shown in ANOVA output in part (b), the $F = \frac{MS_{location}}{MSE}$ = 107.3 and p-value < 0.05. Rejecting null hypothesis
    + Conclusion: We have enough evidence to prove that location affects average hour of time when machine is not in use.

* Main effect for week
    + H0: all $\beta$'s equal 0 vs Ha: at least one $\beta_j \neq 0$  
    + Test Statistics: as shown in ANOVA output in part (b), the $F = \frac{MS_{week}}{MSE}$ = 409.1 and p-value < 0.001. Rejecting null hypothesis
    + Conclusion: We have enough evidence to prove that average hour of time when machine is not in use is different from week to week (week 1 vs week 2).
    
\hfill

**(e)**   Conduct the Tukey’s test for additivity to decide if interaction effects can be ignored at a level of significance $\alpha$ = 0.05. State your conclusions.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
###DEFINE TUKEYS ADD TEST
tukeys.add.test <- function(y,A,B){
## Y is the response vector
## A and B are factors used to predict the mean of y
## Note the ORDER of arguments: Y first, then A and B
   dname <- paste(deparse(substitute(A)), "and", deparse(substitute(B)),
                  "on",deparse(substitute(y)) )
   A <- factor(A); B <- factor(B)
   ybar.. <- mean(y)
   ybari. <- tapply(y,A,mean)
   ybar.j <- tapply(y,B,mean)
   len.means <- c(length(levels(A)), length(levels(B)))
   SSAB <- sum( rep(ybari. - ybar.., len.means[2]) * 
                rep(ybar.j - ybar.., rep(len.means[1], len.means[2])) *
                tapply(y, interaction(A,B), mean))^2 / 
                  ( sum((ybari. - ybar..)^2) * sum((ybar.j - ybar..)^2))
   aovm <- anova(lm(y ~ A+B))
   SSrem <- aovm[3,2] - SSAB
   dfdenom <- aovm[3,1] - 1
    STATISTIC <- SSAB/SSrem*dfdenom
    names(STATISTIC) <- "F"
    PARAMETER <- c(1, dfdenom)
    names(PARAMETER) <- c("num df", "denom df")
    D <- sqrt(SSAB/  ( sum((ybari. - ybar..)^2) * sum((ybar.j - ybar..)^2)))
    names(D) <- "D estimate"
    RVAL <- list(statistic = STATISTIC, parameter = PARAMETER, 
        p.value = 1 - pf(STATISTIC, 1,dfdenom), estimate = D,
        method = "Tukey's one df F test for Additivity", 
        data.name = dname)
    attr(RVAL, "class") <- "htest"
    return(RVAL)
   }
pander(with(data.21, tukeys.add.test(time, location, week)))
####Tukey’s test for additivity adds a term to the ANOVA model that grows proportionately with both the treatment level and the blocking level, that is, (ab)_ij = D * a_i * b_j.  
```

* H0:D = 0 (no interaction) vs Ha:D $\neq$ 0 (there is interaction)
* Since the test statistics F(1,2) = 0.59, p-value > 0.5. We cannot reject the null hypothesis at $\alpha$ = 0.05 significance level. The no-interaction model is plausible. 
* There is no interaction effect of location and week on average hour of time when machine is not in use.

\hfill
\hfill

**Based on KNNL problem 22.13 -22.14 on pages 943-944**. A manufacturer of felt-tip markers investigated by an experiment whether a proposed new display, featuring s picture of a physician, is more effective in drugstores than the present counter display, featuring a picture of an athlete and designed to be located in the stationery area. Fifteen drugstores of similar characteristics were chosen for the study. They were assigned at random in equal numbers to one of the following three treatments: (1) present counter display in stationery area, (2) new display in stationery area, (3) new display in checkout area. Sales with the present display ($X_{ij}$) were recorded in all 15 stores for a three-week period. Then the new display was set up in the 10 drugstores receiving it, and sales for the next three-week period ($Y_{ij}$ ) were recorded in all 15 stores. The data on sales (in dollars) follow.

\hfill

<!---
\begin{tabular}{c c c c c c c} 
& \multicolumn{5} {c} {j}\\ [0.5ex] 
\cline{2-6}
i & 1& 2& 3& 4& 5\\[0.5ex]
\hline
Trt1\\
First 3 weeks & 92 & 68 & 74 & 52 & 65\\
Second 3 weeks & 69 & 44 & 58 & 38 & 54\\[0.5ex]
Trt2\\
First 3 weeks & 77 & 80 & 70 & 73 & 79\\
Second 3 weeks & 74 & 75 & 73 & 78 & 82\\[0.5ex]
Trt3\\
First 3 weeks & 64 & 43 & 81 & 68 & 71\\
Second 3 weeks & 66 & 49 & 84 & 75 & 77\\[0.5ex]
\end{tabular}
-->

<a href="http://tinypic.com?ref=wqwvmr" target="_blank"><img src="http://i62.tinypic.com/wqwvmr.png" border="0" alt="Image and video hosting by TinyPic"></a>

\hfill

Assume that covariance model (22.3) is applicable.

\hfill

**(a)**  Prepare a symbolic scatter plot of the data. Does it appear that there are display effects on mean sales? Discuss

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
###SETTING OVER
data.22=read.table("CH22PR13.txt",col.names=c('Y2','i','j','Y1'))
names(data.22)=c("post","treatment","store",'prior')
data.22$prior.c = scale(data.22$prior, scale = F)
data.22$prior.c = as.numeric(data.22$prior.c)
data.22$treatment = factor(data.22$treatment)
data.22$store = factor(data.22$store)

###PLOT THE CELL MEANS FOR TWO FACTORS  
plot(post~prior, data=data.22, pch=unclass(treatment), main="symbolic scatter plot", xlab = "First 3 Weeks", ylab = "Second 3 Weeks")
legend("topleft", legend=levels(data.22$treatment), pch=c(1:3))
abline(a=0,b=1,col=4,lwd=3)              
```

* Display does appear to have effect on mean sales as almost all display in checkout area (Tr 3) and some of new display in stationery area (Tr 2) have increased their sales during the experimental period (vs Tr 1, present counter display in stationery area).  

\hfill

**(b)**  State the regression model equivalent to covariance model (22.3) for this case; use 1, -1, 0 indicator variables.

* Model: $Y_{ij} = \mu_. + \tau_1 I_{ij1} + \tau_2 I_{ij2} + \gamma (x_{ij} - \bar x_{..}) + \epsilon_{ij}$
* Treatment 1: $I_{ij1} = 1$ and $I_{ij2} = 0$
* Treatment 2: $I_{ij1} = 0$ and $I_{ij2} = 1$
* Treatment 3: $I_{ij1} = -1$ and $I_{ij2} = -1$


\hfill

**(c)**   Obtain ANOVA table

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
# data <- within(data.22, treatment <- relevel(treatment, ref = 3))
model = lm(post ~  treatment, data.22)
panderOptions('round', 4)
pander(anova(model))
```

\hfill

**(d)** Estimate the mean sales with display treatment 2 for stores whose sales in the preceding three-week period were $75; use a 95% confidence interval.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
A = data.22$treatment
x1 = ifelse(A==1,1,0) - ifelse(A==3,1,0)    
x2 = ifelse(A==2,1,0) - ifelse(A==3,1,0)  
data.22 = cbind(data.22, x1, x2)
model2 = lm(post ~  x1 + x2 + prior.c, data.22)
# new <- data.frame(prior.c = 4.53333, treatment = factor(2))
new <- data.frame(x1 = 0, x2 = 1, prior.c = 4.53333)
predict = predict(model2,newdata=new,interval = 'confidence', se.fit = TRUE)
critical.t = qt(0.975,df = 11)
value = predict$fit[1]
se = predict$se.fit
interval = predict$fit[2:3]
panderOptions('round', 2)
pander(cbind('Estimated Value' = value, 'Standard Error' = se, 
             't' = critical.t, '2.5%' = interval[1], '97.5%'=interval[2]))
```

* $\hat Y = \hat \mu_. + \hat \tau_2 + \hat \gamma (75-70.46667)$ = 75.73
* The 95% confidence interval for the estimated second three-week sales is between $71.79 and $79.68.


**(e)** Make all pairwise comparisons between the treatment effects; use the Scheff´e procedure with a 90% family confidence coefficient. State your findings

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
###CREAT SCHEFF'E PROCEDURE TEST
# Scheffe <- function(formula, data, cont, conf.level = 0.90, MSE) 
# {
#     f  <- function(contrast, mean)   {contrast %*% mean}                # (17.41)
#     h  <- function(contrast, n, MSE) {sqrt(MSE * sum(contrast^2 / n))}  # (17.42)
# 
#     df <- model.frame(formula, data)
#     y  <- df[, 1]
#     x  <- df[, 2]
#     r  <- nrow(cont)
# 
#     means <- tapply(y, x, mean)
#     L     <- apply(cont, 2, f, mean = means)
#     se    <- apply(cont, 2, h, n = tapply(y, x, length), MSE = MSE)
#     S     <- sqrt((r-1) * qf(conf.level, r-1, nrow(df)-r))               # (17.43a)
#     cbind(means, L, se, S, L + S*cbind('lwr' = -se, 'upr' = se))                                 # (17.43)
# }
# cont <- matrix(c(1, 0, -1,
#                  0, 1, -1,
#                  1,-1, 0), nrow = 3)
# Scheffe(formula = model2,data= data.22, cont=cont,conf.level = 0.9,MSE= 16.05)
```

* $\hat Y_1 - \hat Y_2$ = $\tau_1 - \tau_2$ = -13.57740 - 5.54806 = -19.12546. 
* $\hat Y_1 - \hat Y_3$ = $\tau_1 - (-\tau_1 - \tau_2)$ = -21.60674
* $\hat Y_2 - \hat Y_3$ = $\tau_2 - (-\tau_1 - \tau_2)$ = -2.48128

\hfill

**(f)** Conduct a test whether or not the treatment effect is significant using $\alpha$ = 0.05. State the hypotheses, P-value and conclusions.

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
model1 = lm(post ~  prior.c, data.22) #reduced
pander(anova(model1,model2))
```

* Full Model: $\hat Y = 66.4 - 13.5774 I_{ij1} + 5.5481 I_{ij2} + 0.83474 x.c$ 
* Reduced Model: $\hat Y = 66.4 + 0.83474 x.c$
* H0: $\tau_1$ = $\tau_2$ = 0 vs Ha: not all of them equal zero
* Test Statistics: By the lack of fit test, the F(2,11) = 45.53, p-value <.001. We reject the null hypothesis.
* We have enough evidence to prove that treatment effect (displays) is significant in predicting sales. 

\hfill

**(g)** Conduct a test whether or not the treatment regression lines have the same slope. State the hypotheses, P-value and conclusions using $\alpha$ = 0.05

```{r, echo=T,message=FALSE,comment=NA, warning=FALSE}
data.22$I1 = data.22$x1*data.22$prior.c
data.22$I2 = data.22$x2*data.22$prior.c
model3 = lm(post ~  x1 + x2 + prior.c + I1 + I2, data.22) # full model
pander(anova(model2,model3))
```

* Full Model: $\hat Y = 66.4 - 13.5774 I_{ij1} + 5.5481 I_{ij2} + 0.83474 x.c - 11.3336 I_{ij1} x.c + 4.631221 I_{ij2} x.c$ 
* Reduced Model: $\hat Y = 66.4 - 13.5774 I_{ij1} + 5.5481 I_{ij2} + 0.83474 x.c$
* H0: $\gamma \tau_1$ = $\gamma \tau_2$ = 0 vs Ha: not all of them equal zero
* Test Statistics: By the lack of fit test, the F(2,9) = 0.971, p-value >0.05. We cannot reject the null hypothesis.
* We don't have enough evidence to conclude an interaction effect. Therefore, all treatment regression lines have the same slope. 

